<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title></title>
<!-- 2014-09-24 Wed 02:59 -->
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="generator" content="Org-mode" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center; }
  .todo   { font-family: monospace; color: red; }
  .done   { color: green; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  pre.src-sh:before    { content: 'sh'; }
  pre.src-bash:before  { content: 'sh'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-R:before     { content: 'R'; }
  pre.src-perl:before  { content: 'Perl'; }
  pre.src-java:before  { content: 'Java'; }
  pre.src-sql:before   { content: 'SQL'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.right  { text-align: center;  }
  th.left   { text-align: center;   }
  th.center { text-align: center; }
  td.right  { text-align: right;  }
  td.left   { text-align: left;   }
  td.center { text-align: center; }
  dt { font-weight: bold; }
  .footpara:nth-child(2) { display: inline; }
  .footpara { display: block; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  /*]]>*/-->
</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/javascript" src="http://orgmode.org/mathjax/MathJax.js"></script>
<script type="text/javascript">
<!--/*--><![CDATA[/*><!--*/
    MathJax.Hub.Config({
        // Only one of the two following lines, depending on user settings
        // First allows browser-native MathML display, second forces HTML/CSS
        //  config: ["MMLorHTML.js"], jax: ["input/TeX"],
            jax: ["input/TeX", "output/HTML-CSS"],
        extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js",
                     "TeX/noUndefined.js"],
        tex2jax: {
            inlineMath: [ ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{displaymath}","\\end{displaymath}"] ],
            skipTags: ["script","noscript","style","textarea","pre","code"],
            ignoreClass: "tex2jax_ignore",
            processEscapes: false,
            processEnvironments: true,
            preview: "TeX"
        },
        showProcessingMessages: true,
        displayAlign: "center",
        displayIndent: "2em",

        "HTML-CSS": {
             scale: 100,
             availableFonts: ["STIX","TeX"],
             preferredFont: "TeX",
             webFont: "TeX",
             imageFont: "TeX",
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    "MML",
                 Firefox: "MML",
                 Opera:   "HTML",
                 other:   "HTML"
             }
        }
    });
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<h1 class="title"></h1>

<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1"><span class="section-number-2">1</span> Neural Networks</h2>
<div class="outline-text-2" id="text-1">
</div>

<div id="outline-container-sec-1-1" class="outline-3">
<h3 id="sec-1-1"><span class="section-number-3">1.1</span> Overview of Neural Networks</h3>
<div class="outline-text-3" id="text-1-1">
<p>
A neural network is a system in which many similar functions, called <i>neurons</i>, are composed together in order to classify inputs, perform some computation,
or approximate some function.
Neural networks are used today in various machine-learning applications such as handwriting- or speech-recognition, and have several interesting mathematical properties.
</p>
</div>

<div id="outline-container-sec-1-1-1" class="outline-4">
<h4 id="sec-1-1-1"><span class="section-number-4">1.1.1</span> Definition of a Neuron</h4>
<div class="outline-text-4" id="text-1-1-1">
<p>
The idea for neural networks is borrowed from the field of biology; the human brain uses cells called neurons for cognition.
Neurons are joined with one another via connections called synapses.
Neurons create voltages based upon the voltages on certain synapses that act as inputs.
Thus when the collective voltage on the input synapses exceeds a certain level the neuron becomes excited and produces a higher voltage on the output synapses.
</p>

<p>
A neuron is modelled mathematically as a function \(f(x_1, x_2, \ldots, x_n) = \phi( \Sigma_{i=1}^n w_ix_i )\), where \(\{w_1, w_2, \ldots\, w_n\}\) is a set of weights,
with each weight \(w_i\) corresponding to an input \(x_i\), and where \(\phi\) is the activation function that determines the output of the neuron based
on the sum of the products of the weights and inputs. For the sake of brevity we will also notate the weight and input vectors as \(\vec{w}\) and \(\vec{x}\), respectively.
\(\Sigma_{i=1}^n w_ix_i\) calculates the collective weighted input; this total is then passed to the activation function \(\phi\) which uses the collective weighted input to generate an output.
</p>

<p>
A neuron could be also thought of more generally as a partial application of \(\phi\) and \(\vec{w}\) over the function \(F( \phi, \vec{w}, \vec{x} )\).
Such a definition may be useful for implementation of a neural network in software.
</p>

<p>
There are two popular activation functions, the perceptron and the sigmoid.
</p>

<p>
Having been invented in 1957 by Frank Rosenblatt, the perceptron was among the first activation functions to be implemented.
It can be used as a standalone classifier on linearly seperable data.
The perceptron can be defined as the function
\[\phi_P(x) = \left\{ \begin{array}{lr} 0 & : x + b \leq 0 \\ 1 & : x + b > 0 \end{array} \right.\]
where \(b\) is a bias that is specific to the neuron.
</p>
<p>
The sigmoid function can be defined as \(\sigma(x) = \frac{1}{1 + e^{-x}}\).
Though bearing some resemblance to the perceptron, the sigmoid function has the advantage of being both smooth and differentiable.
These properties make training a neural network composed of sigmoid neurons much easier then a similar network composed of perceptron neurons.
</p>
</div>
</div>
</div>
<div id="outline-container-sec-1-2" class="outline-3">
<h3 id="sec-1-2"><span class="section-number-3">1.2</span> Demonstration of the Equivalence of Various Perceptron Networks to Certain Boolean Logic Functions</h3>
<div class="outline-text-3" id="text-1-2">
<p>
Neural networks can be used to compute boolean logic functions.
In this section the construction of various logic gates (devices that compute logical functions) from perceptron networks will be demonstrated.
Since traditional computers are constructed from logic gates, predominantly NAND gates, it stands to reason that a neural network could be constructed
to simulate a traditional computer;\footnote{It should be noted, however, that most implementations of neural networks do just the opposite: neurons are simulated in software. This is due to the ease of construction of digital circuitry i.e. transistors over something that requires analog signals like a sigmoid neuron.}
intuitively this leads to the hypothesis that recurrent neural networks are Turing-complete.
This will be proved in the section following the current one.
</p>

<p>
Consider the following network consisting of one perceptron-type neuron, with inputs \(\left(\begin{smallmatrix}x_1 \\ x_2 \end{smallmatrix}\right)\). <br  />
Let \(\vec{w} = \left(\begin{smallmatrix} 1 \\ 1 \end{smallmatrix}\right)\) and \(b=-1\).
Compare the behavior of the network to that of the AND function: <br  />
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="right" />

<col  class="right" />
</colgroup>

<colgroup>
<col  class="right" />

<col  class="right" />

<col  class="right" />
</colgroup>

<colgroup>
<col  class="right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="right">\(x_1\)</th>
<th scope="col" class="right">\(x_2\)</th>
<th scope="col" class="right">\(\vec{x}\cdot\vec{w}\)</th>
<th scope="col" class="right">\(\vec{x}\cdot\vec{w} + b\)</th>
<th scope="col" class="right">Output</th>
<th scope="col" class="right">\(x_1\) AND \(x_2\)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="right">0</td>
<td class="right">0</td>
<td class="right">0</td>
<td class="right">-1</td>
<td class="right">0</td>
<td class="right">0</td>
</tr>

<tr>
<td class="right">0</td>
<td class="right">1</td>
<td class="right">1</td>
<td class="right">0</td>
<td class="right">0</td>
<td class="right">0</td>
</tr>

<tr>
<td class="right">1</td>
<td class="right">0</td>
<td class="right">1</td>
<td class="right">0</td>
<td class="right">0</td>
<td class="right">0</td>
</tr>

<tr>
<td class="right">1</td>
<td class="right">1</td>
<td class="right">2</td>
<td class="right">1</td>
<td class="right">1</td>
<td class="right">1</td>
</tr>
</tbody>
</table>
<p>
Note how the network is equivalent to the AND function over the inputs for which AND is defined.
</p>

<p>
Now let \(\vec{w} = \left(\begin{smallmatrix}1 \\ 1\end{smallmatrix}\right)\) and \(b=0\).
Compare the behavior of the network to that of the OR function: <br  />
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="right" />

<col  class="right" />
</colgroup>

<colgroup>
<col  class="right" />

<col  class="right" />

<col  class="right" />
</colgroup>

<colgroup>
<col  class="right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="right">\(x_1\)</th>
<th scope="col" class="right">\(x_2\)</th>
<th scope="col" class="right">\(\vec{x}\cdot\vec{w}\)</th>
<th scope="col" class="right">\(\vec{x}\cdot\vec{w} + b\)</th>
<th scope="col" class="right">Output</th>
<th scope="col" class="right">\(x_1\) OR  \(x_2\)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="right">0</td>
<td class="right">0</td>
<td class="right">0</td>
<td class="right">0</td>
<td class="right">0</td>
<td class="right">0</td>
</tr>

<tr>
<td class="right">0</td>
<td class="right">1</td>
<td class="right">1</td>
<td class="right">1</td>
<td class="right">1</td>
<td class="right">1</td>
</tr>

<tr>
<td class="right">1</td>
<td class="right">0</td>
<td class="right">1</td>
<td class="right">1</td>
<td class="right">1</td>
<td class="right">1</td>
</tr>

<tr>
<td class="right">1</td>
<td class="right">1</td>
<td class="right">2</td>
<td class="right">2</td>
<td class="right">1</td>
<td class="right">1</td>
</tr>
</tbody>
</table>
<p>
Now, simply by changing the weights, the network output becomes equivalent to the OR function over the domain of OR.
</p>

<p>
Let's also consider NAND.
NAND is important because it is functionally complete, meaning that all boolean expressions can be expressed by an equivalent combination of NANDs.
A proof by exhaustion is available in Appendix A.
</p>
<p>
Now let \(\vec{w} = \left(\begin{smallmatrix} -1 \\ -1 \end{smallmatrix}\right)\) and \(b=2\).
Compare the behavior of the network to that of the NAND function: <br  />
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="right" />

<col  class="right" />
</colgroup>

<colgroup>
<col  class="right" />

<col  class="right" />

<col  class="right" />
</colgroup>

<colgroup>
<col  class="right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="right">\(x_1\)</th>
<th scope="col" class="right">\(x_2\)</th>
<th scope="col" class="right">\(\vec{x}\cdot\vec{w}\)</th>
<th scope="col" class="right">\(\vec{x}\cdot\vec{w} + b\)</th>
<th scope="col" class="right">Output</th>
<th scope="col" class="right">\(x_1\) NAND  \(x_2\)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="right">0</td>
<td class="right">0</td>
<td class="right">0</td>
<td class="right">2</td>
<td class="right">1</td>
<td class="right">1</td>
</tr>

<tr>
<td class="right">0</td>
<td class="right">1</td>
<td class="right">-1</td>
<td class="right">1</td>
<td class="right">1</td>
<td class="right">1</td>
</tr>

<tr>
<td class="right">1</td>
<td class="right">0</td>
<td class="right">-1</td>
<td class="right">1</td>
<td class="right">1</td>
<td class="right">1</td>
</tr>

<tr>
<td class="right">1</td>
<td class="right">1</td>
<td class="right">-2</td>
<td class="right">0</td>
<td class="right">0</td>
<td class="right">0</td>
</tr>
</tbody>
</table>
<p>
Thus a single two-input perceptron can compute NAND.
</p>

<p>
Computer scientists met with some difficulty when attempting to weight a single perceptron to compute the exclusive-or (XOR) function.
In fact, it is impossible to weight a single perceptron to do so (proof left as an exercise to the reader).
</p>
<p>
However, a multilayer perceptron network can be designed to compute XOR.
</p>
</div>
</div>


<div id="outline-container-sec-1-3" class="outline-3">
<h3 id="sec-1-3"><span class="section-number-3">1.3</span> Statement and Proof of the Turing-completeness of Neural Networks</h3>
<div class="outline-text-3" id="text-1-3">
<p>
As was alluded to before, recurrent neural networks are Turing-complete.
The Turing machine is a conceptual machine proposed in 1948 by Alan Turing that can be programmed to compute any possible computer algorithm.
Let \(C\) be a system of computing.
\(C\) is said to be Turing-complete if computers of type \(C\) are capable of simulating any single-taped Turing machine.
Closely related is the idea of Turing equivalence, that a Turing machine can simulate any computer of type \(C\).
Then, by the transitive property, it follows that all Turing-equivalent computers can simulate each other.
</p>
</div>
</div>
<div id="outline-container-sec-1-4" class="outline-3">
<h3 id="sec-1-4"><span class="section-number-3">1.4</span> Statement and Proof of the Universality Theorem</h3>
<div class="outline-text-3" id="text-1-4">
<p>
In the previous section it was shown that recurrent neural networks can compute any function that can be computed by an algorithm.
More impressively, it is also true that a neural setwork with only a single hidden layer can compute any continuous function to an arbitrary degree
of precision. This is known as the Universality Theorem.
</p>
</div>
</div>
<div id="outline-container-sec-1-5" class="outline-3">
<h3 id="sec-1-5"><span class="section-number-3">1.5</span> Statement and Proof of Correctness for a Training Algorithm for a Perceptron</h3>
<div class="outline-text-3" id="text-1-5">
<p>
The power and versatility of neural networks in general was shown in the previous sections;
however, what are the capabilities of a single neuron?
A perceptron neuron can be used to classify linearly seperable data.
In this section a network design and a training algorithm will be stated,
and the ability of the network to correctly classify all data in the training set, provided the training set is linearly seperable, will be proven.
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Created: 2014-09-24 Wed 02:59</p>
<p class="creator"><a href="http://www.gnu.org/software/emacs/">Emacs</a> 24.3.1 (<a href="http://orgmode.org">Org</a> mode 8.0)</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
